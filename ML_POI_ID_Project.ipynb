{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x05B0A8F0>)), ('svc', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87080\tPrecision: 0.84444\tRecall: 0.03800\tF1: 0.07273\tF2: 0.04697\n",
      "\tTotal predictions: 15000\tTrue positives:   76\tFalse positives:   14\tFalse negatives: 1924\tTrue negatives: 12986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline (Scaling, SKB=2, SVC) -> Precision: 0.63060Â¶\n",
    "\n",
    "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=6, score_func=)), ('MultinomialNB', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]) Accuracy: 0.87273 Precision: 0.92523 Recall: 0.04950 F1: 0.09397 F2: 0.06106 Total predictions: 15000 True positives: 99 False positives: 8 False negatives: 1901 True negatives: 12992\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers Before Removal of TOTAL : ['LAY KENNETH L', 'SKILLING JEFFREY K', 'TOTAL']\n",
      "ave_earnings is the average value of: ['salary', 'bonus', 'deferral_payments', 'total_payments']\n",
      "\n",
      " Features List:\n",
      "\n",
      "['poi',\n",
      " 'salary',\n",
      " 'bonus',\n",
      " 'deferral_payments',\n",
      " 'total_payments',\n",
      " 'ave_earnings',\n",
      " 'deferred_income',\n",
      " 'total_stock_value',\n",
      " 'exercised_stock_options',\n",
      " 'restricted_stock',\n",
      " 'restricted_stock_deferred',\n",
      " 'expenses',\n",
      " 'long_term_incentive',\n",
      " 'shared_receipt_with_poi',\n",
      " 'from_this_person_to_poi',\n",
      " 'from_poi_to_this_person',\n",
      " 'to_messages',\n",
      " 'from_messages']\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('skb', SelectKBest(k=10, score_func=<function f_classif at 0x05B0A8F0>)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      " a basic script for importing student's POI identifier,\n",
      "    and checking the results that they get from it \n",
      " \n",
      "    requires that the algorithm, dataset, and features list\n",
      "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
      "    my_feature_list.pkl, respectively\n",
      "\n",
      "    that process should happen at the end of poi_id.py\n",
      "\n",
      "# Tuning hyper-parameters for precision\n",
      "()\n",
      "Best parameters set found on development set:\n",
      "()\n",
      "{'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 2}\n",
      "()\n",
      "Grid scores on development set:\n",
      "()\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 2}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 2}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 2}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 2}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 4}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 4}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 4}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 4}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 5}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 5}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 5}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 5}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 6}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 6}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 6}\n",
      "0.810 (+/-0.045) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 6}\n",
      "()\n",
      "Detailed classification report:\n",
      "()\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.82      1.00      0.90        36\n",
      "        1.0       0.00      0.00      0.00         8\n",
      "\n",
      "avg / total       0.67      0.82      0.74        44\n",
      "\n",
      "()\n",
      "# Tuning hyper-parameters for recall\n",
      "()\n",
      "Best parameters set found on development set:\n",
      "()\n",
      "{'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 2}\n",
      "()\n",
      "Grid scores on development set:\n",
      "()\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 2}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 2}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 2}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 2}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 4}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 4}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 4}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 4}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 5}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 5}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 5}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 5}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 6}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 1000, 'skb__k': 6}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 6}\n",
      "0.900 (+/-0.025) for {'svc__gamma': 0.0001, 'svc__kernel': 'rbf', 'svc__C': 10000, 'skb__k': 6}\n",
      "()\n",
      "Detailed classification report:\n",
      "()\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.82      1.00      0.90        36\n",
      "        1.0       0.00      0.00      0.00         8\n",
      "\n",
      "avg / total       0.67      0.82      0.74        44\n",
      "\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from pprint import pprint\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "# bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "print \"Outliers Before Removal of TOTAL :\",outliers\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "print 'ave_earnings is the average value of:', features_list[1:5]\n",
    "\n",
    "      \n",
    "       \n",
    "###Extract features and labels from dataset for local testing\n",
    "# I removed entries with all 'NaN' values or all '0' in order to clean up data and avoid any problem on calcultions.\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "print \"\\n Features List:\\n\"\n",
    "pprint (features_list)\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "\n",
    "# I tried several different classifiers and their output results of the tester.py script\n",
    "# in the ML_project_varity_of_classifiers.ipny file\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn import cross_validation\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Below is the clasifier with best precision result above .3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.3,random_state=42)\n",
    "skb = SelectKBest()\n",
    "svc = SVC()\n",
    "scaler = MinMaxScaler()\n",
    "clfi = Pipeline(steps=[('scaling',scaler),(\"skb\", skb), (\"svc\", svc)])\n",
    "print clfi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters =  {\n",
    "    'skb__k': [2, 4, 5, 6],\n",
    "    'svc__C': [1000,10000],\n",
    "    'svc__kernel': ['rbf'],\n",
    "    'svc__gamma': [0.001, 0.0001],\n",
    "    }\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    clf = GridSearchCV(clfi, tuned_parameters, cv=3,scoring='%s_weighted' % score)\n",
    "    clf.fit(feature_train, label_train)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    label_true, label_pred = label_test, clf.predict(feature_test)\n",
    "    print(classification_report(label_true, label_pred))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "#test_classifier(clf, my_dataset, features_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers Before Removal of TOTAL : ['LAY KENNETH L', 'SKILLING JEFFREY K', 'TOTAL']\n",
      "ave_earnings is the average value of: ['salary', 'bonus', 'deferral_payments', 'total_payments']\n",
      "\n",
      " Features List:\n",
      "\n",
      "['poi',\n",
      " 'salary',\n",
      " 'bonus',\n",
      " 'deferral_payments',\n",
      " 'total_payments',\n",
      " 'ave_earnings',\n",
      " 'deferred_income',\n",
      " 'total_stock_value',\n",
      " 'exercised_stock_options',\n",
      " 'restricted_stock',\n",
      " 'restricted_stock_deferred',\n",
      " 'expenses',\n",
      " 'long_term_incentive',\n",
      " 'shared_receipt_with_poi',\n",
      " 'from_this_person_to_poi',\n",
      " 'from_poi_to_this_person',\n",
      " 'to_messages',\n",
      " 'from_messages']\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('skb', SelectKBest(k=10, score_func=<function f_classif at 0x05B0A8F0>)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      " a basic script for importing student's POI identifier,\n",
      "    and checking the results that they get from it \n",
      " \n",
      "    requires that the algorithm, dataset, and features list\n",
      "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
      "    my_feature_list.pkl, respectively\n",
      "\n",
      "    that process should happen at the end of poi_id.py\n",
      "\n",
      "# Tuning hyper-parameters for precision\n",
      "()\n",
      "Best parameters set found on development set:\n",
      "()\n",
      "{'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 2}\n",
      "()\n",
      "Grid scores on development set:\n",
      "()\n",
      "0.810 (+/-0.000) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 2}\n",
      "0.810 (+/-0.000) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 2}\n",
      "0.810 (+/-0.000) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 2}\n",
      "0.810 (+/-0.000) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 2}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 4}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 4}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 4}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 4}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 5}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 5}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 5}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 5}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 6}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 6}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 6}\n",
      "0.809 (+/-0.004) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 6}\n",
      "()\n",
      "Detailed classification report:\n",
      "()\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.82      1.00      0.90        36\n",
      "        1.0       0.00      0.00      0.00         8\n",
      "\n",
      "avg / total       0.67      0.82      0.74        44\n",
      "\n",
      "()\n",
      "# Tuning hyper-parameters for recall\n",
      "()\n",
      "Best parameters set found on development set:\n",
      "()\n",
      "{'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 2}\n",
      "()\n",
      "Grid scores on development set:\n",
      "()\n",
      "0.900 (+/-0.000) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 2}\n",
      "0.900 (+/-0.000) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 2}\n",
      "0.900 (+/-0.000) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 2}\n",
      "0.900 (+/-0.000) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 2}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 4}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 4}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 4}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 4}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 5}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 5}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 5}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 5}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 6}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 6}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 6}\n",
      "0.890 (+/-0.040) for {'svc__gamma': 0.0001, 'svc__kernel': 'linear', 'svc__C': 10000, 'skb__k': 6}\n",
      "()\n",
      "Detailed classification report:\n",
      "()\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.82      1.00      0.90        36\n",
      "        1.0       0.00      0.00      0.00         8\n",
      "\n",
      "avg / total       0.67      0.82      0.74        44\n",
      "\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from pprint import pprint\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "# bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "print \"Outliers Before Removal of TOTAL :\",outliers\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "print 'ave_earnings is the average value of:', features_list[1:5]\n",
    "\n",
    "      \n",
    "       \n",
    "###Extract features and labels from dataset for local testing\n",
    "# I removed entries with all 'NaN' values or all '0' in order to clean up data and avoid any problem on calcultions.\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "print \"\\n Features List:\\n\"\n",
    "pprint (features_list)\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "\n",
    "# I tried several different classifiers and their output results of the tester.py script\n",
    "# in the ML_project_varity_of_classifiers.ipny file\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn import cross_validation\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Below is the clasifier with best precision result above .3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.3,random_state=42)\n",
    "skb = SelectKBest()\n",
    "svc = SVC()\n",
    "scaler = MinMaxScaler()\n",
    "clfi = Pipeline(steps=[('scaling',scaler),(\"skb\", skb), (\"svc\", svc)])\n",
    "print clfi\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters =  {\n",
    "    'skb__k': [2, 4, 5, 6],\n",
    "    'svc__C': [1000,10000],\n",
    "    'svc__kernel': ['linear'],\n",
    "    'svc__gamma': [0.001, 0.0001],\n",
    "    }\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    clf = GridSearchCV(clfi, tuned_parameters, cv=5,scoring='%s_weighted' % score)\n",
    "    clf.fit(feature_train, label_train)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    label_true, label_pred = label_test, clf.predict(feature_test)\n",
    "    print(classification_report(label_true, label_pred))\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Best parameters set found on development set:\n",
    "\n",
    "{'svc__gamma': 0.001, 'svc__kernel': 'linear', 'svc__C': 1000, 'skb__k': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('skb', SelectKBest(k=2, score_func=<function f_classif at 0x05B0A8F0>)), ('svc', SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87133\tPrecision: 0.63060\tRecall: 0.08450\tF1: 0.14903\tF2: 0.10220\n",
      "\tTotal predictions: 15000\tTrue positives:  169\tFalse positives:   99\tFalse negatives: 1831\tTrue negatives: 12901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skb = SelectKBest(k=2)\n",
    "svc = SVC(C=1000, gamma=0.001, kernel='linear')\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"skb\", skb), (\"svc\", svc)])\n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
