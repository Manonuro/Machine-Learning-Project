{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting point: GaussianNB classifier\n",
    "GaussianNB()\n",
    "\tAccuracy: 0.82669\tPrecision: 0.33550\tRecall: 0.12900\tF1: 0.18635\tF2: 0.14711\n",
    "\tTotal predictions: 13000\tTrue positives:  258\tFalse positives:  511\tFalse negatives: 1742\tTrue negatives: 10489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salary': 365788, 'to_messages': 807, 'deferral_payments': 'NaN', 'total_payments': 1061827, 'exercised_stock_options': 'NaN', 'bonus': 600000, 'restricted_stock': 585062, 'shared_receipt_with_poi': 702, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 585062, 'expenses': 94299, 'loan_advances': 'NaN', 'from_messages': 29, 'other': 1740, 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'mark.metts@enron.com', 'from_poi_to_this_person': 38}\n",
      "LAY KENNETH L 1072321 7000000\n",
      "SKILLING JEFFREY K 1111258 5600000\n",
      "TOTAL 26704229 97343619\n",
      "['LAY KENNETH L', 'SKILLING JEFFREY K', 'TOTAL']\n",
      "LAY KENNETH L 1072321 7000000\n",
      "SKILLING JEFFREY K 1111258 5600000\n",
      "['LAY KENNETH L', 'SKILLING JEFFREY K']\n",
      " Features List  ['poi', 'salary', 'bonus', 'total_payments']\n",
      "GaussianNB()\n",
      "\tAccuracy: 0.82669\tPrecision: 0.33550\tRecall: 0.12900\tF1: 0.18635\tF2: 0.14711\n",
      "\tTotal predictions: 13000\tTrue positives:  258\tFalse positives:  511\tFalse negatives: 1742\tTrue negatives: 10489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from pprint import pprint\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary','bonus','total_payments'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "print data_dict['METTS MARK']\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        print e, data_dict[e][\"salary\"],  data_dict[e]['bonus']\n",
    "        outliers.append(e)\n",
    "print outliers\n",
    "data_dict.pop('TOTAL',0)\n",
    "#data_dict.pop('LAY KENNETH L',0)\n",
    "#data_dict.pop('SKILLING JEFFREY K',0)\n",
    "#       print e, data_dict[e][\"salary\"],  data_dict[e]['bonus']\n",
    "#key_list = [k for k in data_dict.keys() if data_dict[k][\"salary\"] != 'NaN' and data_dict[k][\"salary\"] > 1000000 and data_dict[k][\"bonus\"] > 5000000]\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        print e, data_dict[e][\"salary\"],  data_dict[e]['bonus']\n",
    "        outliers.append(e)\n",
    "print outliers\n",
    "\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:  http://scikit-lea\n",
    "### rn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSp\n",
    "### lit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "print \" Features List \", features_list\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features_list with  'ave_earnings':\n",
      "['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'restricted_stock', 'restricted_stock_deferred', 'expenses', 'long_term_incentive', 'shared_receipt_with_poi', 'from_this_person_to_poi', 'from_poi_to_this_person', 'to_messages', 'from_messages']\n",
      "GaussianNB()\n",
      "\tAccuracy: 0.78440\tPrecision: 0.28768\tRecall: 0.41800\tF1: 0.34081\tF2: 0.38328\n",
      "\tTotal predictions: 15000\tTrue positives:  836\tFalse positives: 2070\tFalse negatives: 1164\tTrue negatives: 10930\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "#bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "#data_dict.pop('LAY KENNETH L',0)\n",
    "#data_dict.pop('SKILLING JEFFREY K',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "# I replaced all 'NaN' values with '0' in order to clean up data and avoid any problem on the calcultions.\n",
    "\n",
    "for ele in my_dataset:\n",
    "    for e in features_list:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            my_dataset[ele][e] = 0 \n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:  http://scikit-lea\n",
    "### rn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSp\n",
    "### lit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "print \"New features_list with  'ave_earnings':\\n\", features_list\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.931034482759\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Not POI       0.93      1.00      0.96        27\n",
      "        POI       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.87      0.93      0.90        29\n",
      "\n",
      "New features_list with 'ave_earnings':\n",
      "['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'restricted_stock', 'restricted_stock_deferred', 'expenses', 'long_term_incentive', 'shared_receipt_with_poi', 'from_this_person_to_poi', 'from_poi_to_this_person', 'to_messages', 'from_messages']\n",
      "Got a divide by zero when trying out: SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "#bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "#data_dict.pop('LAY KENNETH L',0)\n",
    "#data_dict.pop('SKILLING JEFFREY K',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "# I replaced all 'NaN' values with '0' in order to clean up data and avoid any problem on the calcultions.\n",
    "\n",
    "for ele in my_dataset:\n",
    "    for e in features_list:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            my_dataset[ele][e] = 0 \n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf', C=1000,gamma=.001)\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:  http://scikit-lea\n",
    "### rn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSp\n",
    "### lit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    \n",
    "from sklearn.metrics import classification_report\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "accuracy =clf.score(features_test,labels_test)\n",
    "pred = clf.predict(features_test)\n",
    "target_names = [\"Not POI\", \"POI\"]\n",
    "print \"accuracy:\",accuracy\n",
    "print classification_report(y_true=labels_test, y_pred=pred, target_names=target_names)\n",
    "        \n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "print \"New features_list with 'ave_earnings':\\n\", features_list\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAY KENNETH L 1072321 7000000\n",
      "SKILLING JEFFREY K 1111258 5600000\n",
      "TOTAL 26704229 97343619\n",
      "outliers_before_removal : ['LAY KENNETH L', 'SKILLING JEFFREY K', 'TOTAL']\n",
      "LAY KENNETH L 1072321 7000000\n",
      "SKILLING JEFFREY K 1111258 5600000\n",
      "outliers_after_removal : ['LAY KENNETH L', 'SKILLING JEFFREY K']\n",
      "accuracy: 0.789473684211\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Not POI       0.79      1.00      0.88        30\n",
      "        POI       0.00      0.00      0.00         8\n",
      "\n",
      "avg / total       0.62      0.79      0.70        38\n",
      "\n",
      " Features List  ['poi', 'salary', 'bonus', 'total_payments']\n",
      "Got a divide by zero when trying out: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary','bonus','total_payments'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "#bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        print e, data_dict[e][\"salary\"],  data_dict[e]['bonus']\n",
    "        outliers.append(e)\n",
    "        \n",
    "print \"outliers_before_removal :\",outliers\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "#data_dict.pop('LAY KENNETH L',0)\n",
    "#data_dict.pop('SKILLING JEFFREY K',0)\n",
    "\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        print e, data_dict[e][\"salary\"],  data_dict[e]['bonus']\n",
    "        outliers.append(e)\n",
    "print \"outliers_after_removal :\",outliers\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf', C=1)\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:  http://scikit-lea\n",
    "### rn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSp\n",
    "### lit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "accuracy =clf.score(features_test,labels_test)\n",
    "pred = clf.predict(features_test)\n",
    "target_names = [\"Not POI\", \"POI\"]\n",
    "print \"accuracy:\",accuracy\n",
    "print classification_report(y_true=labels_test, y_pred=pred, target_names=target_names)\n",
    "    \n",
    "    \n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "print \" Features List \", features_list\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features List Data Cleaning\n",
    "The script below is going to finalized features_list for future use in different identifier. The features_list contains selected features from original set of features in the data_dict, plus new feature, 'ave_earnings', and all 'NaN' values replaced with 0 on numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features List: ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'restricted_stock', 'restricted_stock_deferred', 'expenses', 'long_term_incentive', 'shared_receipt_with_poi', 'from_this_person_to_poi', 'from_poi_to_this_person', 'to_messages', 'from_messages']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "#bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "# I replaced all 'NaN' values with '0' in order to clean up data and avoid any problem on the calcultions.\n",
    "\n",
    "for ele in my_dataset:\n",
    "    for e in features_list:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            my_dataset[ele][e] = 0 \n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)            \n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "print \"Features List:\", features_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling and KBest data selection\n",
    "\n",
    "The features_list will be scaled with MinMaxScaler and will go through KBest data selection in different identifiers. Below the selected features after scaling will be shown for K=1 thru 7 which are going to be used in our pipeline procedures for different identifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of KBest selected features_list K=7 :\n",
      "['poi', 'salary', 'bonus', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'long_term_incentive', 'shared_receipt_with_poi']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "kbest = SelectKBest(k=7)\n",
    "kbest.fit_transform(feature_train, label_train)\n",
    "features_new =[] \n",
    "for i in range(len(kbest.get_support())):\n",
    "    if kbest.get_support().tolist()[i] is True:\n",
    "        features_new.append(features_list[i+1])\n",
    "features_list_new = [\"poi\"] + features_new\n",
    "print \"List of KBest selected features_list K=7 :\\n\",features_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of KBest selected features_list K=6 :\n",
      "['poi', 'salary', 'bonus', 'total_stock_value', 'exercised_stock_options', 'long_term_incentive', 'shared_receipt_with_poi']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "kbest = SelectKBest(k=6)\n",
    "kbest.fit_transform(feature_train, label_train)\n",
    "features_new =[] \n",
    "for i in range(len(kbest.get_support())):\n",
    "    if kbest.get_support().tolist()[i] is True:\n",
    "        features_new.append(features_list[i+1])\n",
    "features_list_new = [\"poi\"] + features_new\n",
    "print \"List of KBest selected features_list K=6 :\\n\",features_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of KBest selected features_list K=5 :\n",
      "['poi', 'salary', 'bonus', 'total_stock_value', 'exercised_stock_options', 'long_term_incentive']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "kbest = SelectKBest(k=5)\n",
    "kbest.fit_transform(feature_train, label_train)\n",
    "features_new =[] \n",
    "for i in range(len(kbest.get_support())):\n",
    "    if kbest.get_support().tolist()[i] is True:\n",
    "        features_new.append(features_list[i+1])\n",
    "features_list_new = [\"poi\"] + features_new\n",
    "print \"List of KBest selected features_list K=5 :\\n\",features_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of KBest selected features_list  K=4 :\n",
      "['poi', 'salary', 'bonus', 'total_stock_value', 'exercised_stock_options']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "kbest = SelectKBest(k=4)\n",
    "kbest.fit_transform(feature_train, label_train)\n",
    "features_new =[] \n",
    "for i in range(len(kbest.get_support())):\n",
    "    if kbest.get_support().tolist()[i] is True:\n",
    "        features_new.append(features_list[i+1])\n",
    "features_list_new = [\"poi\"] + features_new\n",
    "print \"List of KBest selected features_list  K=4 :\\n\",features_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of KBest selected features_list  K=3 :\n",
      "['poi', 'salary', 'bonus', 'total_stock_value']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "kbest = SelectKBest(k=3)\n",
    "kbest.fit_transform(feature_train, label_train)\n",
    "features_new =[] \n",
    "for i in range(len(kbest.get_support())):\n",
    "    if kbest.get_support().tolist()[i] is True:\n",
    "        features_new.append(features_list[i+1])\n",
    "features_list_new = [\"poi\"] + features_new\n",
    "print \"List of KBest selected features_list  K=3 :\\n\",features_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of KBest selected features_list  K=2 :\n",
      "['poi', 'salary', 'bonus']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "kbest = SelectKBest(k=2)\n",
    "kbest.fit_transform(feature_train, label_train)\n",
    "features_new =[] \n",
    "for i in range(len(kbest.get_support())):\n",
    "    if kbest.get_support().tolist()[i] is True:\n",
    "        features_new.append(features_list[i+1])\n",
    "features_list_new = [\"poi\"] + features_new\n",
    "print \"List of KBest selected features_list  K=2 :\\n\",features_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of KBest selected features_list  K=1 :\n",
      "['poi', 'bonus']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split(\n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "kbest = SelectKBest(k=1)\n",
    "kbest.fit_transform(feature_train, label_train)\n",
    "features_new =[] \n",
    "for i in range(len(kbest.get_support())):\n",
    "    if kbest.get_support().tolist()[i] is True:\n",
    "        features_new.append(features_list[i+1])\n",
    "features_list_new = [\"poi\"] + features_new\n",
    "print \"List of KBest selected features_list  K=1 :\\n\",features_list_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline method with Scaler, SelectKBest, and GaussianNB ()\n",
    "Comparing the precision result for identifiers configured Pipeline method with Scaler, SelectKBest, and GaussianNB () with K=4,5,and 6 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=6, score_func=<function f_classif at 0x085A82F0>)), ('NaiveBayes', GaussianNB())])\n",
      "\tAccuracy: 0.84620\tPrecision: 0.39921\tRecall: 0.30400\tF1: 0.34516\tF2: 0.31923\n",
      "\tTotal predictions: 15000\tTrue positives:  608\tFalse positives:  915\tFalse negatives: 1392\tTrue negatives: 12085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Pipeline method with Scaler, SelectKBest (K=6), and GaussianNB () \n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=6)\n",
    "gnb = GaussianNB()\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"NaiveBayes\", gnb)])\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=5, score_func=<function f_classif at 0x085A82F0>)), ('NaiveBayes', GaussianNB())])\n",
      "\tAccuracy: 0.85300\tPrecision: 0.43492\tRecall: 0.34250\tF1: 0.38322\tF2: 0.35770\n",
      "\tTotal predictions: 15000\tTrue positives:  685\tFalse positives:  890\tFalse negatives: 1315\tTrue negatives: 12110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Pipeline method with Scaler, SelectKBest (K=5), and GaussianNB () \n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=5)\n",
    "gnb = GaussianNB()\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"NaiveBayes\", gnb)])\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=4, score_func=<function f_classif at 0x085A82F0>)), ('NaiveBayes', GaussianNB())])\n",
      "\tAccuracy: 0.85033\tPrecision: 0.42152\tRecall: 0.32900\tF1: 0.36956\tF2: 0.34411\n",
      "\tTotal predictions: 15000\tTrue positives:  658\tFalse positives:  903\tFalse negatives: 1342\tTrue negatives: 12097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=5), and GaussianNB () \n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=4)\n",
    "gnb = GaussianNB()\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"NaiveBayes\", gnb)])\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best result for Pipeline method with Scaler, SelectKBest, and GaussianNB () is for k=5 with Precision: 0.43492\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline method with Scaler, SelectKBest, and MultinomialNB()\n",
    "Comparing the precision result for identifiers configured Pipeline method with Scaler, SelectKBest, and MultinomialNB() with K=2 thru 6 values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Best result for Pipeline method with Scaler, SelectKBest, and MultinomialNB():\n",
    "\n",
    "Precision: 1.00000 is obtainable for K values from 3 to 5 but they seem too good to be true.\n",
    "\n",
    "List of KBest selected features_list  K=6 ['poi', 'salary', 'bonus', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'expenses']\n",
    " Precision: 0.90741\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=3, score_func=<function f_classif at 0x085A82F0>)), ('MultinomialNB', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "\tAccuracy: 0.86793\tPrecision: 1.00000\tRecall: 0.00950\tF1: 0.01882\tF2: 0.01185\n",
      "\tTotal predictions: 15000\tTrue positives:   19\tFalse positives:    0\tFalse negatives: 1981\tTrue negatives: 13000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=2), and MultinomialNB () \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.3,random_state=42)\n",
    "skb = SelectKBest(k=3)\n",
    "mnb = MultinomialNB()\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"MultinomialNB\", mnb)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=5, score_func=<function f_classif at 0x085A82F0>)), ('MultinomialNB', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "\tAccuracy: 0.87247\tPrecision: 1.00000\tRecall: 0.04350\tF1: 0.08337\tF2: 0.05379\n",
      "\tTotal predictions: 15000\tTrue positives:   87\tFalse positives:    0\tFalse negatives: 1913\tTrue negatives: 13000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=5), and MultinomialNB () \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=5)\n",
    "mnb = MultinomialNB()\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"MultinomialNB\", mnb)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=6, score_func=<function f_classif at 0x085A82F0>)), ('MultinomialNB', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "\tAccuracy: 0.87253\tPrecision: 0.90741\tRecall: 0.04900\tF1: 0.09298\tF2: 0.06043\n",
      "\tTotal predictions: 15000\tTrue positives:   98\tFalse positives:   10\tFalse negatives: 1902\tTrue negatives: 12990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=6), and MultinomialNB () \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=6)\n",
    "mnb = MultinomialNB()\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"MultinomialNB\", mnb)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=7, score_func=<function f_classif at 0x085A82F0>)), ('MultinomialNB', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "\tAccuracy: 0.87173\tPrecision: 0.79688\tRecall: 0.05100\tF1: 0.09586\tF2: 0.06275\n",
      "\tTotal predictions: 15000\tTrue positives:  102\tFalse positives:   26\tFalse negatives: 1898\tTrue negatives: 12974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=7), and MultinomialNB () \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=7)\n",
    "mnb = MultinomialNB()\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"MultinomialNB\", mnb)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline method with Scaler, SelectKBest, and SVC()\n",
    "Comparing the precision result for identifiers configured Pipeline method with Scaler, SelectKBest, and SVC() with K=1 thru 4 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=1, score_func=<function f_classif at 0x085A82F0>)), ('SVC', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86680\tPrecision: 0.50694\tRecall: 0.03650\tF1: 0.06810\tF2: 0.04482\n",
      "\tTotal predictions: 15000\tTrue positives:   73\tFalse positives:   71\tFalse negatives: 1927\tTrue negatives: 12929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=4), and svm() \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=1)\n",
    "svm = SVC(kernel='rbf', C=1)\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"SVC\", svm)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=2, score_func=<function f_classif at 0x085A82F0>)), ('SVC', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87080\tPrecision: 0.84444\tRecall: 0.03800\tF1: 0.07273\tF2: 0.04697\n",
      "\tTotal predictions: 15000\tTrue positives:   76\tFalse positives:   14\tFalse negatives: 1924\tTrue negatives: 12986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=4), and svm() \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=2)\n",
    "svm = SVC(kernel='rbf', C=1)\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"SVC\", svm)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers Before Removal of TOTAL : ['LAY KENNETH L', 'SKILLING JEFFREY K', 'TOTAL']\n",
      "ave_earnings is the average value of: ['salary', 'bonus', 'deferral_payments', 'total_payments']\n",
      "\n",
      " Features List:\n",
      "\n",
      "['poi',\n",
      " 'salary',\n",
      " 'bonus',\n",
      " 'deferral_payments',\n",
      " 'total_payments',\n",
      " 'ave_earnings',\n",
      " 'deferred_income',\n",
      " 'total_stock_value',\n",
      " 'exercised_stock_options',\n",
      " 'restricted_stock',\n",
      " 'restricted_stock_deferred',\n",
      " 'expenses',\n",
      " 'long_term_incentive',\n",
      " 'shared_receipt_with_poi',\n",
      " 'from_this_person_to_poi',\n",
      " 'from_poi_to_this_person',\n",
      " 'to_messages',\n",
      " 'from_messages']\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=2, score_func=<function f_classif at 0x085A82F0>)), ('SVC', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87080\tPrecision: 0.84444\tRecall: 0.03800\tF1: 0.07273\tF2: 0.04697\n",
      "\tTotal predictions: 15000\tTrue positives:   76\tFalse positives:   14\tFalse negatives: 1924\tTrue negatives: 12986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from pprint import pprint\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "# bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "print \"Outliers Before Removal of TOTAL :\",outliers\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "print 'ave_earnings is the average value of:', features_list[1:5]\n",
    "\n",
    "      \n",
    "       \n",
    "###Extract features and labels from dataset for local testing\n",
    "# I removed entries with all 'NaN' values or all '0' in order to clean up data and avoid any problem on calcultions.\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "print \"\\n Features List:\\n\"\n",
    "pprint (features_list)\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "\n",
    "# I tried several different classifiers and their output results of the tester.py script\n",
    "# in the ML_project_varity_of_classifiers.ipny file\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn import cross_validation\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Below is the clasifier with best precision result above .3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=2)\n",
    "svm = SVC(kernel='rbf', C=1)\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"SVC\", svm)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=3, score_func=<function f_classif at 0x085A82F0>)), ('SVC', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86773\tPrecision: 0.78571\tRecall: 0.01100\tF1: 0.02170\tF2: 0.01370\n",
      "\tTotal predictions: 15000\tTrue positives:   22\tFalse positives:    6\tFalse negatives: 1978\tTrue negatives: 12994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=4), and svm() \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=3)\n",
    "svm = SVC(kernel='rbf', C=1)\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"SVC\", svm)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=4, score_func=<function f_classif at 0x085A82F0>)), ('SVC', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86693\tPrecision: 0.75000\tRecall: 0.00300\tF1: 0.00598\tF2: 0.00375\n",
      "\tTotal predictions: 15000\tTrue positives:    6\tFalse positives:    2\tFalse negatives: 1994\tTrue negatives: 12998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=4), and svm() \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=4)\n",
    "svm = SVC(kernel='rbf', C=1)\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"SVC\", svm)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=2, score_func=<function f_classif at 0x085A82F0>)), ('SVC', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=1000, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.86673\tPrecision: 0.50276\tRecall: 0.04550\tF1: 0.08345\tF2: 0.05562\n",
      "\tTotal predictions: 15000\tTrue positives:   91\tFalse positives:   90\tFalse negatives: 1909\tTrue negatives: 12910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=4), and svm() \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=2)\n",
    "svm = SVC(kernel='rbf',C=1.000, max_iter=1000,degree=3,gamma=1)\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"SVC\", svm)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=2, score_func=<function f_classif at 0x085A82F0>)), ('SVC', SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=1, gamma='auto', kernel='linear',\n",
      "  max_iter=10000, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.87133\tPrecision: 0.63060\tRecall: 0.08450\tF1: 0.14903\tF2: 0.10220\n",
      "\tTotal predictions: 15000\tTrue positives:  169\tFalse positives:   99\tFalse negatives: 1831\tTrue negatives: 12901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "###Pipeline method with Scaler, SelectKBest (K=4), and svm() \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=2)\n",
    "svm = SVC(C=1000, max_iter=10000,degree=1, kernel='linear',gamma=\"auto\")\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"SVC\", svm)])\n",
    "\n",
    "\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best result for Pipeline method with Scaler, SelectKBest, and SVC () is :\n",
    "\n",
    "List of KBest selected features_list  K=2 :['poi', 'total_stock_value', 'exercised_stock_options']\n",
    "Precision: 0.84444  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "I will check PCA with different parameters to find the best features among 18 seleted features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.93497604e-01   1.62548991e-01   1.91840911e-02   1.00914934e-02\n",
      "   7.19056441e-03   4.22065416e-03   2.02841480e-03   1.02980131e-03\n",
      "   1.12341912e-04   6.15343671e-05   2.35941988e-05   1.08774198e-05\n",
      "   2.83270452e-08   8.91942962e-09   7.94354765e-10   1.77275871e-11\n",
      "   1.30617092e-11   4.66936034e-16]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "#bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "def doPCA():\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=None, copy=True, whiten=False)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "pca = doPCA()\n",
    "print pca.explained_variance_ratio_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best parameters set found on SVC development set:\n",
    "()\n",
    "{'kernel': 'rbf', 'C': 100, 'gamma': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LAY KENNETH L', 'SKILLING JEFFREY K', 'TOTAL']\n",
      "ave_earnings is the average value of ['salary', 'bonus', 'deferral_payments', 'total_payments']\n",
      "\n",
      " Features List:\n",
      "\n",
      "['poi',\n",
      " 'salary',\n",
      " 'bonus',\n",
      " 'deferral_payments',\n",
      " 'total_payments',\n",
      " 'ave_earnings',\n",
      " 'deferred_income',\n",
      " 'total_stock_value',\n",
      " 'exercised_stock_options',\n",
      " 'restricted_stock',\n",
      " 'restricted_stock_deferred',\n",
      " 'expenses',\n",
      " 'long_term_incentive',\n",
      " 'shared_receipt_with_poi',\n",
      " 'from_this_person_to_poi',\n",
      " 'from_poi_to_this_person',\n",
      " 'to_messages',\n",
      " 'from_messages']\n",
      " a basic script for importing student's POI identifier,\n",
      "    and checking the results that they get from it \n",
      " \n",
      "    requires that the algorithm, dataset, and features list\n",
      "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
      "    my_feature_list.pkl, respectively\n",
      "\n",
      "    that process should happen at the end of poi_id.py\n",
      "\n",
      "# Tuning hyper-parameters for precision\n",
      "()\n",
      "Best parameters set found on development set:\n",
      "()\n",
      "{'kernel': 'rbf', 'C': 100, 'gamma': 1}\n",
      "()\n",
      "Grid scores on development set:\n",
      "()\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 100, 'gamma': 1}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.1}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 1000, 'gamma': 1}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.1}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 10000, 'gamma': 1}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 10000, 'gamma': 0.1}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 10000, 'gamma': 0.001}\n",
      "0.757 (+/-0.069) for {'kernel': 'rbf', 'C': 10000, 'gamma': 0.0001}\n",
      "()\n",
      "Detailed classification report:\n",
      "()\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      1.00      0.94        39\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.79      0.89      0.83        44\n",
      "\n",
      "()\n",
      "# Tuning hyper-parameters for recall\n",
      "()\n",
      "Best parameters set found on development set:\n",
      "()\n",
      "{'kernel': 'rbf', 'C': 100, 'gamma': 1}\n",
      "()\n",
      "Grid scores on development set:\n",
      "()\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 100, 'gamma': 1}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.1}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 1000, 'gamma': 1}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.1}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 10000, 'gamma': 1}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 10000, 'gamma': 0.1}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 10000, 'gamma': 0.001}\n",
      "0.870 (+/-0.040) for {'kernel': 'rbf', 'C': 10000, 'gamma': 0.0001}\n",
      "()\n",
      "Detailed classification report:\n",
      "()\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      1.00      0.94        39\n",
      "        1.0       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.79      0.89      0.83        44\n",
      "\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from pprint import pprint\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "# bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "print outliers\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "print 'ave_earnings is the average value of', features_list[1:5]\n",
    "\n",
    "      \n",
    "       \n",
    "###Extract features and labels from dataset for local testing\n",
    "# I removed entries with all 'NaN' values or all '0' in order to clean up data and avoid any problem on calcultions.\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "print \"\\n Features List:\\n\"\n",
    "pprint (features_list)\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "\n",
    "# I tried several different classifiers and their output results of the tester.py script\n",
    "# in the ML_project_varity_of_classifiers.ipny file\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn import cross_validation\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Below is the clasifier with best precision result above .3\n",
    "\n",
    "\n",
    "\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.3,random_state=42)\n",
    "\n",
    "#from __future__ import print_function\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1, 1e-1, 1e-3, 1e-4],'C': [100,1000, 10000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,scoring='%s_weighted' % score)\n",
    "    clf.fit(feature_train, label_train)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    label_true, label_pred = label_test, clf.predict(feature_test)\n",
    "    print(classification_report(label_true, label_pred))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA with default parametr values, returns the percentage of variance explained by all of the components. In next examination I try n_components=4 for the first four componenets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7934976   0.16254899  0.01918409  0.01009149]\n"
     ]
    }
   ],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "def doPCA():\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=4, copy=True, whiten=False)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "pca = doPCA()\n",
    "print pca.explained_variance_ratio_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7934976   0.16254899  0.01918409  0.01009149]\n"
     ]
    }
   ],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "def doPCA():\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=4, copy=True, whiten=True)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "pca = doPCA()\n",
    "print pca.explained_variance_ratio_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the the default values for (copy=True, whiten=True) did not change the result and the explained_variance_ratio_ was the same. In the next steps I will try different combinations of PCA and clasifiers for n_components=2 as they have highest explained_variance_ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with PCA and GaussianNB()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=2, whiten=False)), ('gaussiannb', GaussianNB())])\n",
      "\tAccuracy: 0.87073\tPrecision: 0.52837\tRecall: 0.28400\tF1: 0.36943\tF2: 0.31295\n",
      "\tTotal predictions: 15000\tTrue positives:  568\tFalse positives:  507\tFalse negatives: 1432\tTrue negatives: 12493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [('reduce_dim', PCA(n_components=2)), ('gaussiannb', GaussianNB())]\n",
    "clf = Pipeline(estimators)\n",
    " \n",
    "    \n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=1, whiten=False)), ('gaussiannb', GaussianNB())])\n",
      "\tAccuracy: 0.87860\tPrecision: 0.61854\tRecall: 0.23350\tF1: 0.33902\tF2: 0.26670\n",
      "\tTotal predictions: 15000\tTrue positives:  467\tFalse positives:  288\tFalse negatives: 1533\tTrue negatives: 12712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [('reduce_dim', PCA(n_components=1)), ('gaussiannb', GaussianNB())]\n",
    "clf = Pipeline(estimators)\n",
    " \n",
    "    \n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=4, whiten=False)), ('gaussiannb', GaussianNB())])\n",
      "\tAccuracy: 0.86513\tPrecision: 0.48891\tRecall: 0.25350\tF1: 0.33388\tF2: 0.28051\n",
      "\tTotal predictions: 15000\tTrue positives:  507\tFalse positives:  530\tFalse negatives: 1493\tTrue negatives: 12470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [('reduce_dim', PCA(n_components=4)), ('gaussiannb', GaussianNB())]\n",
    "clf = Pipeline(estimators)\n",
    " \n",
    "    \n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Best result for pipeline with PCA (n_components=1) and GaussianNB():\n",
    "Precision: 0.61854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline with PCA and SVM\n",
    "I will use pipeline to chain PCA and SVM for the identifier and then will use grid search to see the effect of different parameter setup for each one of them onthe output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=None, whiten=False)), ('svm', SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "estimators = [('reduce_dim', PCA()), ('svm', SVC(C=1000,gamma=.001))]\n",
    "clf = Pipeline(estimators)\n",
    "clf\n",
    "#SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, \n",
    "#tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, random_state=None)\n",
    "\n",
    "  \n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### read in data dictionary, convert to numpy array\n",
    "data_dict = pickle.load( open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "features_list = ['salary', 'bonus', 'deferral_payments', 'total_payments', \n",
    "                 'deferred_income','total_stock_value']\n",
    "#data = featureFormat(data_dict, features)\n",
    "#print data\n",
    "\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True, remove_NaN=True,\n",
    "                     remove_all_zeroes=True, remove_any_zeroes=False)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "### your code below\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    matplotlib.pyplot.scatter( salary, bonus )\n",
    "\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('reduce_dim', PCA(n_components=2)), ('svm', SVC(kernel='rbf', C=1))]\n",
    "clf = Pipeline(estimators)\n",
    "print clf \n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def doPCA():\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "pca = doPCA()\n",
    "print pca.explained_variance_ratio_\n",
    "first_pc = pca.components_[0]\n",
    "second_pc = pca.components_[1]\n",
    "\n",
    "transformed_data = pca.transform(data)\n",
    "for ii, jj in zip(transformed_data, data):\n",
    "    plt.scatter( first_pc[0]*ii[0], first_pc[1]*ii[0], color=\"r\")\n",
    "    plt.scatter( second_pc[0]*ii[1], second_pc[1]*ii[1], color=\"c\")\n",
    "\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "#from sklearn import cross_validation\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "#X_train[0], X_test[0], y_train[0], y_test[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "#test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84148041  0.14221172  0.00931718  0.00536635]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def doPCA():\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(data)\n",
    "    return pca\n",
    "\n",
    "pca = doPCA()\n",
    "print pca.explained_variance_ratio_\n",
    "first_pc = pca.components_[0]\n",
    "second_pc = pca.components_[1]\n",
    "\n",
    "\n",
    "#from sklearn import cross_validation\n",
    "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "#X_train[0], X_test[0], y_train[0], y_test[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "#test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=2, whiten=False)), ('svm', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Got a divide by zero when trying out: Pipeline(steps=[('reduce_dim', PCA(copy=True, n_components=2, whiten=False)), ('svm', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "estimators = [('reduce_dim', PCA(n_components=2)), ('svm', SVC(kernel='rbf', C=1))]\n",
    "clf = Pipeline(estimators)\n",
    "print clf \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.2, random_state=20)\n",
    "    \n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=5, score_func=<function f_classif at 0x0562F070>)), ('NaiveBayes', GaussianNB())])\n",
      "\tAccuracy: 0.85300\tPrecision: 0.43492\tRecall: 0.34250\tF1: 0.38322\tF2: 0.35770\n",
      "\tTotal predictions: 15000\tTrue positives:  685\tFalse positives:  890\tFalse negatives: 1315\tTrue negatives: 12110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "#bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "#    print earnings, np.mean(earnings)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "# I replaced all 'NaN' values with '0' in order to clean up data and avoid any problem on the calcultions.\n",
    "\n",
    "for ele in my_dataset:\n",
    "    for e in features_list:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            my_dataset[ele][e] = 0 \n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=5)\n",
    "gnb = GaussianNB()\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"NaiveBayes\", gnb)])\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list) \n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but mgake sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=6, score_func=<function f_classif at 0x0562F070>)), ('MultinomialNB', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "\tAccuracy: 0.87253\tPrecision: 0.90741\tRecall: 0.04900\tF1: 0.09298\tF2: 0.06043\n",
      "\tTotal predictions: 15000\tTrue positives:   98\tFalse positives:   10\tFalse negatives: 1902\tTrue negatives: 12990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "#bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=6)\n",
    "mnb = MultinomialNB()\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"MultinomialNB\", mnb)])\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "test_classifier(clf, my_dataset, features_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('SKB', SelectKBest(k=6, score_func=<function f_classif at 0x0562F070>)), ('SVC', SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.85493\tPrecision: 0.34058\tRecall: 0.09400\tF1: 0.14734\tF2: 0.10992\n",
      "\tTotal predictions: 15000\tTrue positives:  188\tFalse positives:  364\tFalse negatives: 1812\tTrue negatives: 12636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus', 'deferral_payments', 'total_payments', 'ave_earnings', \n",
    "                 'deferred_income','total_stock_value', 'exercised_stock_options', \n",
    "                'restricted_stock', 'restricted_stock_deferred', 'expenses',  \n",
    "                 'long_term_incentive', 'shared_receipt_with_poi', \n",
    "                 'from_this_person_to_poi','from_poi_to_this_person',\n",
    "                'to_messages','from_messages'] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "# List of all keys of the data_dict for  salary value > 1 million and \n",
    "#bonus > 5 million dollars\n",
    "outliers = []\n",
    "for e in data_dict.keys():\n",
    "    if data_dict[e][\"salary\"] != 'NaN' and data_dict[e]['salary'] > 1000000 and data_dict[e]['bonus'] > 5000000:\n",
    "        outliers.append(e)\n",
    "        \n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "#### I will add a new feature that shows average value of total earning called ave_earnings \n",
    "#### by calculating the mean value of 'salary', 'bonus', 'deferral_payments', and 'total_payments' for each person.\n",
    "for ele in my_dataset:\n",
    "    earnings = []\n",
    "    for e in features_list[1:5]:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            earn = 0\n",
    "            earnings.append(earn)\n",
    "        earnings.append(earn)\n",
    "#    print earnings, np.mean(earnings)\n",
    "    ave_earnings = np.mean(earnings)\n",
    "    my_dataset[ele].update({'ave_earnings': ave_earnings})\n",
    "\n",
    "# I replaced all 'NaN' values with '0' in order to clean up data and avoid any problem on the calcultions.\n",
    "\n",
    "for ele in my_dataset:\n",
    "    for e in features_list:\n",
    "        earn = my_dataset[ele][e]\n",
    "        if earn =='NaN':\n",
    "            my_dataset[ele][e] = 0 \n",
    "\n",
    "\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "feature_train, feature_test, label_train, label_test = train_test_split( \n",
    "features, labels, test_size=0.2,random_state=42)\n",
    "skb = SelectKBest(k=6)\n",
    "svm = SVC(kernel='rbf', C=1000)\n",
    "scaler = MinMaxScaler()\n",
    "clf = Pipeline(steps=[('scaling',scaler),(\"SKB\", skb), (\"SVC\", svm)])\n",
    "\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "\n",
    "\n",
    "#from sklearn import cross_validation\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "test_classifier(clf, my_dataset, features_list) \n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "\tAccuracy: 0.82669\tPrecision: 0.33550\tRecall: 0.12900\tF1: 0.18635\tF2: 0.14711\n",
      "\tTotal predictions: 13000\tTrue positives:  258\tFalse positives:  511\tFalse negatives: 1742\tTrue negatives: 10489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
